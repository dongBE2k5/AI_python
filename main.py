import os
import faiss
import requests
import json
from fastapi import FastAPI
from pydantic import BaseModel
from dotenv import load_dotenv
from contextlib import asynccontextmanager

from llama_index.core import StorageContext, load_index_from_storage, Settings
from llama_index.vector_stores.faiss import FaissVectorStore
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from fastapi.middleware.cors import CORSMiddleware

load_dotenv()

STORAGE_DIR = "storage"
OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY")
YOUR_SITE_URL = os.getenv("YOUR_SITE_URL")
YOUR_SITE_NAME = os.getenv("YOUR_SITE_NAME")

# --- ƒê·ªíNG B·ªò EMBEDDING V·ªöI build_index.py ---
Settings.embed_model = HuggingFaceEmbedding(
    model_name="sentence-transformers/all-MiniLM-L12-v2"
)

# --- H√ÄM M·ªû R·ªòNG C√ÇU H·ªéI (QUERY EXPANSION) ---
def expand_queries(original_query):
    """S·ª≠ d·ª•ng LLM ƒë·ªÉ t·∫°o ra 6 bi·∫øn th·ªÉ c·ªßa c√¢u h·ªèi nh·∫±m t·ªëi ∆∞u h√≥a vi·ªác t√¨m ki·∫øm."""
    headers = {
        "Authorization": f"Bearer {OPENROUTER_API_KEY}",
        "Content-Type": "application/json",
        "HTTP-Referer": f"{YOUR_SITE_URL}",
        "X-Title": f"{YOUR_SITE_NAME}",
    }
    
    prompt_expansion = f"""B·∫°n l√† m·ªôt chuy√™n gia tra c·ª©u t√†i li·ªáu t·∫°i Tr∆∞·ªùng Cao ƒë·∫≥ng C√¥ng ngh·ªá Th·ªß ƒê·ª©c (TDC). 
    T·ª´ c√¢u h·ªèi g·ªëc c·ªßa ng∆∞·ªùi d√πng, h√£y t·∫°o ra 6 c√¢u h·ªèi bi·∫øn th·ªÉ c√≥ c√πng √Ω nghƒ©a nh∆∞ng ƒë·∫ßy ƒë·ªß t·ª´ kh√≥a h∆°n (v√≠ d·ª• th√™m nƒÉm h·ªçc 2025-2026, t√™n tr∆∞·ªùng) ƒë·ªÉ h·ªó tr·ª£ t√¨m ki·∫øm ch√≠nh x√°c.
    
    C√¢u h·ªèi g·ªëc: "{original_query}"
    
    Y√™u c·∫ßu tr·∫£ v·ªÅ: CH·ªà tr·∫£ v·ªÅ danh s√°ch c√°c c√¢u h·ªèi, m·ªói c√¢u m·ªôt d√≤ng, kh√¥ng ƒë√°nh s·ªë, kh√¥ng th√™m vƒÉn b·∫£n d·∫´n nh·∫≠p."""

    payload = {
        "model": "deepseek/deepseek-r1-0528:free", # D√πng b·∫£n free ƒë·ªÉ ti·∫øt ki·ªám token m·ªü r·ªông
        "messages": [{"role": "user", "content": prompt_expansion}],
        "max_tokens": 500,
        "temperature": 0.8
    }
    
    try:
        response = requests.post("https://openrouter.ai/api/v1/chat/completions", headers=headers, data=json.dumps(payload))
        res_json = response.json()
        content = res_json['choices'][0]['message']['content'].strip()
        lines = content.split('\n')
        expanded_list = [original_query] + [line.strip() for line in lines if line.strip()]
        return expanded_list[:6] # L·∫•y t·ªëi ƒëa 6 c√¢u (g·ªëc + 5 ph·ª•)
    except Exception as e:
        print(f"‚ö†Ô∏è L·ªói m·ªü r·ªông c√¢u h·ªèi: {e}")
        return [original_query]

@asynccontextmanager
async def lifespan(app: FastAPI):
    faiss_path = os.path.join(STORAGE_DIR, "faiss.index")
    
    if not os.path.exists(faiss_path):
        raise FileNotFoundError(f"Kh√¥ng th·∫•y {faiss_path}. Ch·∫°y 'python build_index.py' tr∆∞·ªõc!")

    # 1. Load FAISS & Index
    faiss_index = faiss.read_index(faiss_path)
    vector_store = FaissVectorStore(faiss_index=faiss_index)
    storage_context = StorageContext.from_defaults(vector_store=vector_store, persist_dir=STORAGE_DIR)
    index = load_index_from_storage(storage_context)

    # 2. L∆∞u retriever v√†o app.state (M·ªói c√¢u truy xu·∫•t 2 ƒëo·∫°n ƒë·ªÉ bao ph·ªß t·ªët h∆°n)
    app.state.retriever = index.as_retriever(similarity_top_k=2)
    
    print("‚úÖ H·ªá th·ªëng RAG (Multi-Query) ƒë√£ s·∫µn s√†ng!")
    yield

app = FastAPI(lifespan=lifespan)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], 
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class ChatRequest(BaseModel):
    prompt: str

@app.post("/api/chat")
async def chat(req: ChatRequest):
    # B∆∞·ªõc 1: Sinh c√°c bi·∫øn th·ªÉ c√¢u h·ªèi
    queries = expand_queries(req.prompt)
    print(f"üîç ƒêang truy xu·∫•t v·ªõi c√°c c√¢u h·ªèi: {queries}")

    # B∆∞·ªõc 2: Truy xu·∫•t t√†i li·ªáu cho t·∫•t c·∫£ c√°c c√¢u h·ªèi
    all_nodes = []
    for q in queries:
        nodes = app.state.retriever.retrieve(q)
        all_nodes.extend(nodes)

    # B∆∞·ªõc 3: Lo·∫°i b·ªè n·ªôi dung tr√πng l·∫∑p (Deduplication)
    unique_contents = {}
    for node in all_nodes:
        # D√πng content l√†m key ƒë·ªÉ tr√°nh tr√πng l·∫∑p th√¥ng tin
        unique_contents[node.get_content()[:200]] = node.get_content()
    
    context_text = "\n---\n".join(unique_contents.values())

    # B∆∞·ªõc 4: X√¢y d·ª±ng Prompt cu·ªëi c√πng
    system_prompt = f"""   B·∫°n l√† m·ªôt tr·ª£ l√Ω t∆∞ v·∫•n chuy√™n nghi·ªáp, t·∫≠n t√¢m.
    Nhi·ªám v·ª• DUY NH·∫§T c·ªßa b·∫°n l√† gi·∫£i ƒë√°p th·∫Øc m·∫Øc d·ª±a tr√™n th√¥ng tin trong [NG·ªÆ C·∫¢NH T√ÄI LI·ªÜU] ƒë∆∞·ª£c cung c·∫•p d∆∞·ªõi ƒë√¢y.
---
  [NG·ªÆ C·∫¢NH T√ÄI LI·ªÜU START]
{context_text}
    [NG·ªÆ C·∫¢NH T√ÄI LI·ªÜU END]
---

  QUY T·∫ÆC TR·∫¢ L·ªúI NGHI√äM NG·∫∂T (B·∫ÆT BU·ªòC TU√ÇN TH·ª¶):
    1. **PH·∫†M VI:** CH·ªà ƒë∆∞·ª£c s·ª≠ d·ª•ng th√¥ng tin c√≥ trong [NG·ªÆ C·∫¢NH T√ÄI LI·ªÜU]. 
       - TUY·ªÜT ƒê·ªêI KH√îNG s·ª≠ d·ª•ng ki·∫øn th·ª©c b√™n ngo√†i (ki·∫øn th·ª©c hu·∫•n luy·ªán tr∆∞·ªõc ƒë√≥) ƒë·ªÉ th√™m th·∫Øt th√¥ng tin kh√¥ng c√≥ trong vƒÉn b·∫£n.
       - N·∫øu th√¥ng tin ng∆∞·ªùi d√πng h·ªèi KH√îNG c√≥ trong t√†i li·ªáu, h√£y tr·∫£ l·ªùi th·∫≥ng th·∫Øn v√† l·ªãch s·ª±: "Xin l·ªói, trong t√†i li·ªáu t√¥i ƒë∆∞·ª£c cung c·∫•p hi·ªán kh√¥ng c√≥ th√¥ng tin v·ªÅ v·∫•n ƒë·ªÅ n√†y."

    2. **T·ªîNG H·ª¢P TH√îNG TIN:**
       - ƒê·ªëi v·ªõi c√¢u h·ªèi v·ªÅ s·ªë l∆∞·ª£ng ho·∫∑c danh s√°ch (v√≠ d·ª•: "c√≥ bao nhi√™u ng√†nh", "g·ªìm nh·ªØng g√¨"), h√£y tr·∫£ l·ªùi t·ªïng qu√°t v√† t·ª± nhi√™n (v√≠ d·ª•: "D·∫°, theo t√†i li·ªáu th√¨ tr∆∞·ªùng c√≥ kho·∫£ng X ng√†nh, ti√™u bi·ªÉu l√† A, B, C..."). 
       - Tr√°nh li·ªát k√™ danh s√°ch d√†i d√≤ng tr·ª´ khi ƒë∆∞·ª£c y√™u c·∫ßu c·ª• th·ªÉ.

    3. **PHONG C√ÅCH:** 
       - Gi·ªçng vƒÉn t·ª± nhi√™n, th√¢n thi·ªán, gi·ªëng ng∆∞·ªùi t∆∞ v·∫•n th·∫≠t s·ª±.
       - D√πng t·ª´ ng·ªØ l·ªãch s·ª± (D·∫°, Th∆∞a, B·∫°n...).

    H√£y nh·ªõ: S·ª± ch√≠nh x√°c theo t√†i li·ªáu l√† ∆∞u ti√™n h√†ng ƒë·∫ßu."""

    headers = {
        "Authorization": f"Bearer {OPENROUTER_API_KEY}",
        "Content-Type": "application/json",
        "HTTP-Referer": f"{YOUR_SITE_URL}",
        "X-Title": f"{YOUR_SITE_NAME}",
    }
    print("ü§ñ ƒêang g·ª≠i y√™u c·∫ßu ƒë·∫øn OpenRouter LLM...", system_prompt)
    payload = {
        "model": "google/gemini-2.5-flash-lite", 
        "messages": [
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": f"{system_prompt}\n\nC√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng: {req.prompt}"
                    }
                ]
            }
        ],
        "max_tokens": 1000,
        "temperature": 0.2 # Gi·∫£m xu·ªëng ƒë·ªÉ ƒë·∫£m b·∫£o t√≠nh ch√≠nh x√°c cho RAG
    }

    response = requests.post(
        url="https://openrouter.ai/api/v1/chat/completions",
        headers=headers,
        data=json.dumps(payload)
    )

    result = response.json()
    
    try:
        reply = result['choices'][0]['message']['content']
    except (KeyError, IndexError):
        reply = "C√≥ l·ªói khi k·∫øt n·ªëi v·ªõi AI: " + str(result)

    return {"reply": reply}

@app.get("/")
def health():
    return {"status": "ok"}